{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration code for \"Building a complete RL system\" lecture\n",
    "\n",
    "## Introduction\n",
    "This code demonstrates our implementation of SARSA for a deterministic FrozenLake task and serves as additional information to go alongside the \"Building a complete Rl system\" lecture.\n",
    "The lecture is delivered as part of the [Reinforcement Learning (2020) course](http://www.drps.ed.ac.uk/19-20/dpt/cxinfr11010.html) at the University of Edinburgh.\n",
    "\n",
    "## Environment: FrozenLake ... but less frozen\n",
    "Let's first define our deterministic FrozenLake environment. Usually, ice is pretty slippery ... but we prefer it stable. Hence, we remove any slipping and stochasticity and comfortably solve the newly created, deterministic task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "\n",
    "# register non-slippery/ deterministic FrozenLake environment\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Interface\n",
    "In order to get familiar with the task, we first create an interface such that a human player can play the game. In the end, playing is fun! And it helps us to understand the task at hand. So let's go ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for human player\n",
    "import contextlib\n",
    "import termios\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def raw_mode(file):\n",
    "    old_attrs = termios.tcgetattr(file.fileno())\n",
    "    new_attrs = old_attrs[:]\n",
    "    new_attrs[3] = new_attrs[3] & ~(termios.ECHO | termios.ICANON)\n",
    "    try:\n",
    "        termios.tcsetattr(file.fileno(), termios.TCSADRAIN, new_attrs)\n",
    "        yield\n",
    "    finally:\n",
    "        termios.tcsetattr(file.fileno(), termios.TCSADRAIN, old_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_action(act: str) -> int:\n",
    "    \"\"\"\n",
    "    Input transferred to action id (for FrozenLake)\n",
    "\n",
    "    :param act (int): received input\n",
    "    :return (int): action id for FrozenLake (and -1 for ESC)\n",
    "    \"\"\"\n",
    "    if act == \"A\":\n",
    "        return 0\n",
    "    elif act == \"S\":\n",
    "        return 1\n",
    "    elif act == \"D\":\n",
    "        return 2\n",
    "    elif act == \"W\":\n",
    "        return 3\n",
    "    elif act == \"STOP\":\n",
    "        return -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown input {act}!\")\n",
    "        \n",
    "def get_input():\n",
    "    act = input(\"Choose action [WASD | STOP]\")\n",
    "    if act == \"STOP\":\n",
    "        return -1\n",
    "    else:\n",
    "        return str_to_action(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def human_player(env: gym.Env):\n",
    "    \"\"\"\n",
    "    Play FrozenLake as a human player with WASD keys\n",
    "    \"\"\"\n",
    "    print(\"Use WASD to move in the environment and end game with ESC or keyboard interrupt (Ctrl-C)\")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "\n",
    "    while True:\n",
    "        act = get_input()\n",
    "        \n",
    "        if act == -1:\n",
    "            return\n",
    "        _, rew, done, _ = env.step(act)\n",
    "        env.render()\n",
    "        if done:\n",
    "            if rew == 1:\n",
    "                print(\"EPISODE FINISHED - SOLVED\")\n",
    "            else:\n",
    "                print(\"EPISODE FINISHED - FAILED\")\n",
    "            env.reset()\n",
    "            env.render()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use WASD to move in the environment and end game with ESC or keyboard interrupt (Ctrl-C)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Choose action [WASD | STOP]STOP\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "human_player(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Agent\n",
    "Now after understanding the task of FrozenLake, let's implement an agent for the on-policy TD control algorithm, also called SARSA. If you need a revision, have a look at [lecture 6  on Temporal Difference Learning](https://www.learn.ed.ac.uk/bbcswebdav/pid-4067850-dt-content-rid-11604420_1/xid-11604420_1) (Slides 15 - 17) or take a look at [section 6.4 in the RL book](http://www.incompleteideas.net/book/RLbook2018.pdf##page=153) on the SARSA method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation tools for final Q-tables\n",
    "def visualise_q_table(q_table):\n",
    "    \"\"\"\n",
    "    Print q_table in human-readable format\n",
    "\n",
    "    :param q_table (Dict): q_table in form of a dict mapping (observation, action) pairs to\n",
    "        q-values\n",
    "    \"\"\"\n",
    "    for key in sorted(q_table.keys()):\n",
    "        obs, act = key\n",
    "        act_name = act_to_str(act)\n",
    "        q_value = q_table[key]\n",
    "        print(f\"Pos={obs}\\tAct={act_name}\\t->\\t{q_value}\")\n",
    "\n",
    "def act_to_str(act: int):\n",
    "    if act == 0:\n",
    "        return \"L\"\n",
    "    elif act == 1:\n",
    "        return \"D\"\n",
    "    elif act == 2:\n",
    "        return \"R\"\n",
    "    elif act == 3:\n",
    "        return \"U\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action value\")\n",
    "        \n",
    "def visualise_policy(q_table):\n",
    "    \"\"\"\n",
    "    Given q_table print greedy policy for each FrozenLake position\n",
    "\n",
    "    :param q_table (Dict): q_table in form of a dict mapping (observation, action) pairs to\n",
    "        q-values\n",
    "    \"\"\"\n",
    "    # extract best acts\n",
    "    act_table = np.zeros((4,4))\n",
    "    str_table = []\n",
    "    for row in range(4):\n",
    "        str_table.append(\"\")\n",
    "        for col in range(4):\n",
    "            pos = row * 4 + col\n",
    "            max_q = None\n",
    "            max_a = None\n",
    "            for a in range(4):\n",
    "                q = q_table[(pos, a)]\n",
    "                if max_q is None or q > max_q:\n",
    "                    max_q = q\n",
    "                    max_a = a\n",
    "            act_table[row, col] = max_a\n",
    "            str_table[row] += act_to_str(max_a)\n",
    "    \n",
    "    # print best actions in human_readable format\n",
    "    print(\"\\nAction selection table:\")\n",
    "    for row_str in str_table:\n",
    "        print(row_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import DefaultDict\n",
    "\n",
    "\n",
    "class SARSA(ABC):\n",
    "    \"\"\"Base class for SARSA agent\n",
    "\n",
    "    :attr n_acts (int): number of actions\n",
    "    :attr gamma (float): discount factor gamma\n",
    "    :attr epsilon (float): epsilon hyperparameter for epsilon-greedy policy\n",
    "    :attr alpha (float): learning rate alpha for updates\n",
    "    :attr q_table (DefaultDict): table for Q-values mapping (OBS, ACT) pairs of observations\n",
    "        and actions to respective Q-values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_acts: int,\n",
    "            gamma: float,\n",
    "            epsilon: float = 0.9,\n",
    "            alpha: float = 0.1\n",
    "        ):\n",
    "        \"\"\"Constructor for SARSA agent\n",
    "\n",
    "        Initializes basic variables of the agent namely the epsilon, learning rate and discount\n",
    "        rate.\n",
    "\n",
    "        :param num_acts (int): number of possible actions\n",
    "        :param gamma (float): discount factor (gamma)\n",
    "        :param epsilon (float): initial epsilon for epsilon-greedy action selection\n",
    "        :param alpha (float): learning rate alpha\n",
    "        \"\"\"\n",
    "        self.n_acts: int = num_acts\n",
    "        self.gamma: float = gamma\n",
    "        self.epsilon: float = epsilon\n",
    "        self.alpha: float = alpha\n",
    "\n",
    "        self.q_table: DefaultDict = defaultdict(lambda: 0)\n",
    "\n",
    "    def act(self, obs: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection \n",
    "\n",
    "        :param obs (np.ndarray of float with dim (observation size)):\n",
    "            received observation representing the current environmental state\n",
    "        :return (int): index of selected action\n",
    "        \"\"\"\n",
    "        act_vals = [self.q_table[(obs, act)] for act in range(self.n_acts)]\n",
    "        max_val = max(act_vals)\n",
    "        max_acts = [idx for idx, act_val in enumerate(act_vals) if act_val == max_val]\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_acts - 1)\n",
    "        else:\n",
    "            return random.choice(max_acts)\n",
    "\n",
    "    def learn(\n",
    "            self,\n",
    "            obs: np.ndarray,\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            n_obs: np.ndarray,\n",
    "            n_action: int,\n",
    "            done: bool\n",
    "        ) -> float:\n",
    "        \"\"\"Updates the Q-table based on agent experience\n",
    "\n",
    "        :param obs (np.ndarray of float with dim (observation size)):\n",
    "            received observation representing the current environmental state\n",
    "        :param action (int): index of applied action\n",
    "        :param reward (float): received reward\n",
    "        :param n_obs (np.ndarray of float with dim (observation size)):\n",
    "            received observation representing the next environmental state\n",
    "        :param done (bool): flag indicating whether a terminal state has been reached\n",
    "        :return (float): updated Q-value for current observation-action pair\n",
    "        \"\"\"\n",
    "        target_value = reward + self.gamma * (1 - done) * self.q_table[(n_obs, n_action)]\n",
    "        self.q_table[(obs, action)] += self.alpha * (\n",
    "            target_value - self.q_table[(obs, action)]\n",
    "        )\n",
    "        return self.q_table[(obs, action)]\n",
    "\n",
    "    def schedule_hyperparameters(self, timestep: int, max_timestep: int):\n",
    "        \"\"\"Updates the hyperparameters\n",
    "\n",
    "        This function is called before every episode and allows you to schedule your\n",
    "        hyperparameters.\n",
    "\n",
    "        :param timestep (int): current timestep at the beginning of the episode\n",
    "        :param max_timestep (int): maximum timesteps that the training loop will run for\n",
    "        \"\"\"\n",
    "        self.epsilon = 1.0-(min(1.0, timestep/(0.07*max_timestep)))*0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "CONFIG = {\n",
    "    \"env\": \"FrozenLakeNotSlippery-v0\",\n",
    "    \"target_reward\": 1,\n",
    "    \"eval_solved_goal\": 10,\n",
    "    \"total_eps\": 1000,\n",
    "    \"eval_episodes\": 1,\n",
    "    \"eval_freq\": 10,\n",
    "    \"gamma\": 0.99,\n",
    "    \"alpha\": 0.1,\n",
    "    \"epsilon\": 0.9,\n",
    "}\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "def evaluate(env, config, q_table, eval_episodes=10, render=False, output=True):\n",
    "    \"\"\"\n",
    "    Evaluate configuration of SARSA on given environment initialised with given Q-table\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :param q_table (Dict[(Obs, Act), float]): Q-table mapping observation-action to Q-values\n",
    "    :param eval_episodes (int): number of evaluation episodes\n",
    "    :param render (bool): flag whether evaluation runs should be rendered\n",
    "    :param output (bool): flag whether mean evaluation performance should be printed\n",
    "    :return (float, float): mean and standard deviation of reward received over episodes\n",
    "    \"\"\"\n",
    "    eval_agent = SARSA(\n",
    "            num_acts=env.action_space.n,\n",
    "            gamma=config[\"gamma\"],\n",
    "            epsilon=0.0, \n",
    "            alpha=config[\"alpha\"],\n",
    "    )\n",
    "    eval_agent.q_table = q_table\n",
    "    episodic_rewards = []\n",
    "    for eps_num in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        if render:\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            act = eval_agent.act(obs)\n",
    "            n_obs, reward, done, info = env.step(act)\n",
    "            if render:\n",
    "                env.render()\n",
    "                sleep(1)\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "            obs = n_obs\n",
    "\n",
    "        episodic_rewards.append(episodic_reward)\n",
    "\n",
    "    mean_reward = np.mean(episodic_rewards)\n",
    "    std_reward = np.std(episodic_rewards)\n",
    "\n",
    "    if output:\n",
    "        print(f\"EVALUATION: MEAN REWARD OF {mean_reward}\")\n",
    "        if mean_reward == 1.0:\n",
    "            print(f\"EVALUATION: SOLVED\")\n",
    "        else:\n",
    "            print(f\"EVALUATION: NOT SOLVED!\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def train(env, config, output=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate SARSA on given environment with provided hyperparameters\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :param output (bool): flag if mean evaluation results should be printed\n",
    "    :return (float, List[float], List[float], Dict[(Obs, Act), float]):\n",
    "        total reward over all episodes, list of means and standard deviations of evaluation\n",
    "        rewards, final Q-table\n",
    "    \"\"\"\n",
    "    agent = SARSA(\n",
    "            num_acts=env.action_space.n,\n",
    "            gamma=config[\"gamma\"],\n",
    "            epsilon=config[\"epsilon\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "    )\n",
    "\n",
    "    step_counter = 0\n",
    "    max_steps = config[\"total_eps\"]\n",
    "    \n",
    "    total_reward = 0\n",
    "    evaluation_reward_means = []\n",
    "    evaluation_reward_stds = []\n",
    "    eval_solved = 0\n",
    "\n",
    "    for eps_num in range(config[\"total_eps\"]):\n",
    "        obs = env.reset()\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # take first action\n",
    "        act = agent.act(obs)\n",
    "\n",
    "        while not done:\n",
    "            n_obs, reward, done, info = env.step(act)\n",
    "            step_counter += 1\n",
    "            episodic_reward += reward\n",
    "\n",
    "            agent.schedule_hyperparameters(step_counter, max_steps)\n",
    "            n_act = agent.act(n_obs)\n",
    "            agent.learn(obs, act, reward, n_obs, n_act, done)\n",
    "\n",
    "            obs = n_obs\n",
    "            act = n_act\n",
    "\n",
    "        total_reward += episodic_reward\n",
    "\n",
    "        if eps_num > 0 and eps_num % config[\"eval_freq\"] == 0:\n",
    "            mean_reward, std_reward = evaluate(\n",
    "                    env,\n",
    "                    config,\n",
    "                    agent.q_table,\n",
    "                    eval_episodes=config[\"eval_episodes\"],\n",
    "                    render=RENDER,\n",
    "                    output=output\n",
    "            )\n",
    "            evaluation_reward_means.append(mean_reward)\n",
    "            evaluation_reward_stds.append(std_reward)\n",
    "\n",
    "            if mean_reward >= config[\"target_reward\"]:\n",
    "                eval_solved += 1\n",
    "                if output:\n",
    "                    print(f\"Reached reward {mean_reward} >= {config['target_reward']} (target reward)\")\n",
    "                if eval_solved == config[\"eval_solved_goal\"]:\n",
    "                    if output:\n",
    "                        print(f\"Solved evaluation {eval_solved} times in a row --> terminate training\")\n",
    "                    break\n",
    "            else:\n",
    "                eval_solved = 0\n",
    "\n",
    "    return total_reward, evaluation_reward_means, evaluation_reward_stds, agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: MEAN REWARD OF 0.0\n",
      "EVALUATION: NOT SOLVED!\n",
      "EVALUATION: MEAN REWARD OF 0.0\n",
      "EVALUATION: NOT SOLVED!\n",
      "EVALUATION: MEAN REWARD OF 0.0\n",
      "EVALUATION: NOT SOLVED!\n",
      "EVALUATION: MEAN REWARD OF 0.0\n",
      "EVALUATION: NOT SOLVED!\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "EVALUATION: MEAN REWARD OF 1.0\n",
      "EVALUATION: SOLVED\n",
      "Reached reward 1.0 >= 1 (target reward)\n",
      "Solved evaluation 10 times in a row --> terminate training\n",
      "\n",
      "\n",
      "Action selection table:\n",
      "RRDL\n",
      "LLDL\n",
      "LLDL\n",
      "LRRL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(CONFIG[\"env\"])\n",
    "    \n",
    "total_reward, _, _, q_table = train(env, CONFIG)\n",
    "# print(\"Q-table:\")\n",
    "# visualise_q_table(q_table)\n",
    "print()\n",
    "visualise_policy(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
